{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Modeling the Data\n",
    "#### Angela Jiang, Alexander Lin, Jason Shen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from datetime import timedelta\n",
    "from sklearn.linear_model import LogisticRegression as LogReg\n",
    "from sklearn.linear_model import LogisticRegressionCV as LogRegCV\n",
    "import math\n",
    "import string \n",
    "from six.moves.html_parser import HTMLParser\n",
    "import urllib2\n",
    "import json\n",
    "import time\n",
    "from functools import wraps\n",
    "from copy import deepcopy\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier as RandomForest\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consolidated Preliminary Functions ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def filter_data(data, option = 'headline', date_filter = None, word_filter = None):\n",
    "    \"\"\" Helper function that filters data by dates and/or words\n",
    "    data = pandas dataframe with columns 'pub_date', 'headline', 'paragraph'\n",
    "    option = 'headline' or 'paragraph' (default = 'headline')\n",
    "    date_filer = (start date, end date) inclusive; else if default = None, then no filter \n",
    "    word_filter = list of words to filter OPTION by (CASE SENSITIVE); default is None\n",
    "    pos = list of parts of speech tags (default = None)\n",
    "    stem = boolean, whether or not to stem (default = False)\"\"\"\n",
    "    \n",
    "    h = HTMLParser()\n",
    "    \n",
    "    # filter by dates\n",
    "    if date_filter is not None:\n",
    "        start_date, end_date = date_filter\n",
    "        filtered_data = data[(data['publish_date'] >= start_date) & (data['publish_date'] <= end_date)]\n",
    "    else:\n",
    "        filtered_data = data\n",
    "        \n",
    "    \n",
    "    # filter by words\n",
    "    if word_filter is not None:\n",
    "        idx_to_drop = [] # store indices of rows that do not contain filter words\n",
    "        \n",
    "        # for every article\n",
    "        for i in range(filtered_data.shape[0]):\n",
    "            text = filtered_data.iloc[i][option]\n",
    "            \n",
    "            # iterates through each filter word\n",
    "            filter_flag = 0\n",
    "            # if there is no text (type is not string)\n",
    "            if isinstance(text, basestring) is False:\n",
    "                idx_to_drop.append(i)\n",
    "                continue\n",
    "            for word in word_filter:\n",
    "                if word in text:\n",
    "                    filter_flag = 1\n",
    "                    break\n",
    "            # if no filter words in text, drop\n",
    "            if filter_flag == 0:\n",
    "                idx_to_drop.append(i)\n",
    "    \n",
    "    # drops rows without words in filter\n",
    "    filtered_data = filtered_data.drop(filtered_data.index[idx_to_drop])\n",
    "    \n",
    "    # remove caps\n",
    "    filtered_data[option] = filtered_data[option].str.lower()\n",
    "    \n",
    "    # remove punctuation\n",
    "    # remove html encoding puncuation from old news\n",
    "    html_encoding = ['&#8217;', '&#8212;', '&#038;', '&#8230;', '&#8220;', '&#8221;']\n",
    "    for i in range(filtered_data.shape[0]):\n",
    "        for encoding in html_encoding:\n",
    "            if encoding in filtered_data.iloc[i][option]:\n",
    "                index = filtered_data.index[i]\n",
    "                filtered_data.loc[index, option] = filtered_data.loc[index, option].replace(encoding, h.unescape(encoding))\n",
    "    \n",
    "    # remove other punctuation\n",
    "    punctuation = list(',.!@#$%^&*()\\'\\\"`:;?' + u'\\u2018' + u'\\u2019')\n",
    "    for c in punctuation:\n",
    "        filtered_data[option] = filtered_data[option].str.replace(c, '')\n",
    "        \n",
    "    return filtered_data\n",
    "\n",
    "def filter_misc(series, pos = None, stem = False):\n",
    "    \"\"\" Helper function to filter the series text by POS tags and/or to stem the words in the text \n",
    "    series = the pandas series to filter\n",
    "    pos = list of parts of speech tags to filter  (default = None)\n",
    "    stem = boolean, whether or not to stem (default = False) \"\"\"\n",
    "    new_series = pd.Series(index = series.index)\n",
    "    \n",
    "    if stem == True:\n",
    "        # does not stem stopwords\n",
    "        stemmer = EnglishStemmer(ignore_stopwords = True)\n",
    "    \n",
    "    \n",
    "    if pos is not None:\n",
    "        for index, text in enumerate(series):\n",
    "            new_series.iloc[index] = ' '.join([y for y,tag in nltk.pos_tag(nltk.word_tokenize(text)) if tag in pos])\n",
    "        pos_flag = True\n",
    "    else:\n",
    "        pos_flag = False\n",
    "        \n",
    "    if stem is True:\n",
    "        # if both pos and stem\n",
    "        if pos_flag == True:\n",
    "            use_series = new_series\n",
    "        # just stem\n",
    "        else:\n",
    "            use_series = series\n",
    "        for index, text in enumerate(use_series):\n",
    "            text_list = text.split()\n",
    "            stemmed_text = []\n",
    "            for word in text_list:\n",
    "                stemmed_text += [stemmer.stem(word)]\n",
    "            new_series.iloc[index] = ' '.join(stemmed_text)\n",
    "            \n",
    "    return new_series\n",
    "\n",
    "def vectorize(series, threshold_low = 0, threshold_high = None):\n",
    "    \"\"\" Helper function to vectorize headlines into bags of words\n",
    "    series = filtered headlines\n",
    "    threshold_low is the minimum amount of count a word must have to be included (inclusive)\n",
    "    threshold_high is the maximum count a word may have and still be included (inclusive) \"\"\"\n",
    "    \n",
    "    if threshold_high is not None:\n",
    "        vectorizer = CountVectorizer(stop_words = 'english', min_df = threshold_low + 1, max_df = threshold_high)\n",
    "    else:\n",
    "        vectorizer = CountVectorizer(stop_words = 'english', min_df = threshold_low + 1)\n",
    "        \n",
    "    vectorized = vectorizer.fit_transform(series).toarray()\n",
    "    feat_names = vectorizer.get_feature_names()\n",
    "    \n",
    "    return vectorized, feat_names\n",
    "\n",
    "def label_data(filtered_df, threshold_quantity=0.005, threshold_label = 'lt'):\n",
    "    \"\"\" Derive the response variable \n",
    "    threshold_quantity = the percentage of change in the previous stock price \n",
    "                        that is required to be considered a change\n",
    "    threshold_label = 'lt' or 'gt'; indicating that the targeted direction of change is in the negative\n",
    "                                    or positive direction, respectively \"\"\"\n",
    "    df_fil = deepcopy(filtered_df)\n",
    "    \n",
    "    results = []\n",
    "    before_p = []\n",
    "    after_p = []\n",
    "    price_list = []\n",
    "\n",
    "    # iterate through news article's weekday along with corresponding published date\n",
    "    for date in df_fil['pub_date']:\n",
    "        compare_date_after = find_nearest_biz_day(stock_price, date, 'after')\n",
    "        compare_date_before = find_nearest_biz_day(stock_price, date, 'before')\n",
    "        if compare_date_after is None:\n",
    "            print 'No after:', date\n",
    "        if compare_date_before is None:\n",
    "            print 'No before:', date\n",
    "            \n",
    "        price_after = stock_price[stock_price['date'] == compare_date_after]['close'].values\n",
    "        price_before = stock_price[stock_price['date'] == compare_date_before]['close'].values\n",
    "        price_diff = price_after[0] - price_before[0]\n",
    "            \n",
    "        # if targeted direction of change is in the negative direction (i.e., requires a significant decrease)\n",
    "        if(threshold_label == 'lt'):\n",
    "            threshold = threshold_quantity * -1*price_before[0]\n",
    "            # significant decrease in stock price by threshold amount, encode binary 0\n",
    "            if price_diff <= threshold:\n",
    "                results += [0]\n",
    "            # any increase or no significant decrease in stock price, encode binary 1\n",
    "            else:\n",
    "                results += [1]\n",
    "        # if targeted direction of change is in the positive direction (i.e., requires a significant increase)\n",
    "        elif threshold_label == 'gt':\n",
    "            threshold = threshold_quantity * price_before[0]\n",
    "            # significant increase in stock price by threshold amount, encode binary 1\n",
    "            if price_diff >= threshold:\n",
    "                results += [1]\n",
    "            # any decrease or no significant increase in stock price, encode binary 0\n",
    "            else:\n",
    "                results += [0]\n",
    "        else:\n",
    "            print \"incorrect threshold_label input\"\n",
    "\n",
    "        # true before and after values\n",
    "        before_p += [price_before[0]]\n",
    "        after_p += [price_after[0]]\n",
    "\n",
    "    df_fil['price_before'] = before_p\n",
    "    df_fil['price_after'] = after_p\n",
    "    df_fil['price_diff'] = df_fil['price_after'] - df_fil['price_before']\n",
    "    df_fil['y'] = results\n",
    "    \n",
    "    return df_fil\n",
    "\n",
    "def find_nearest_biz_day(stock_df, date, time_type, counter = 0):\n",
    "    \"\"\" Function returns the nearest business date closest to the given date\n",
    "        stock_df: a dataframe\n",
    "        date: a date in date format \n",
    "        time_type: \"before\" or \"after\"\n",
    "            \"before\": finds the nearest business day before the given date\n",
    "            \"after\": finds the nearest business day on or after the given date \"\"\"\n",
    "    \n",
    "    # sanity check - if recurses more than a depth of 10, there is an issue\n",
    "    if counter > 10:\n",
    "        return None\n",
    "    \n",
    "    price = stock_df[stock_df['date'] == date]['close'].values\n",
    "    \n",
    "    # finds the nearest business day on or after the given date\n",
    "    if time_type == 'after':\n",
    "        # if date exists (i.e., not weekend or holiday)\n",
    "        if price.size != 0:\n",
    "            return date\n",
    "        # if weekend or holiday\n",
    "        else:\n",
    "            new_date = date + timedelta(days = 1)\n",
    "            counter += 1\n",
    "            return find_nearest_biz_day(stock_df, date + timedelta(days = 1), 'after', counter)\n",
    "    # finds the nearest business day before the given date\n",
    "    elif time_type == 'before' or time_type == 'before and done':\n",
    "        day_before_price = stock_df[stock_df['date'] == date - timedelta(days = 1)]['close'].values\n",
    "        # if date exists (i.e., not weekend or holiday)\n",
    "        if price.size != 0 and day_before_price != 0 and time_type == 'before':\n",
    "            return date - timedelta(days = 1)\n",
    "        elif price.size != 0 and time_type == 'before and done':\n",
    "            return date\n",
    "        # if weekend or holiday\n",
    "        else:\n",
    "            counter+= 1\n",
    "            return find_nearest_biz_day(stock_df, date - timedelta(days = 1), 'before and done', counter)\n",
    "\n",
    "def make_pca(bag_to_use, pca_components):\n",
    "    \"\"\" Helper function to use PCA with different number of components on bag of words\"\"\"\n",
    "    pca = PCA(n_components = pca_components)\n",
    "    pca.fit(bag_to_use)\n",
    "    return pca.transform(bag_to_use)\n",
    "\n",
    "def score(model, x_test, y_test): \n",
    "    \"\"\" Function that computes the accuracy a given model on the entire test set, \n",
    "    the accuracy on class 0 in the test set\n",
    "    and the accuracy on class 1 in the test set. \n",
    "    Returns a pandas Series \"\"\"\n",
    "    accuracy_overall = model.score(x_test, y_test)\n",
    "    accuracy_class0 = model.score(x_test[y_test == 0], y_test[y_test == 0])\n",
    "    accuracy_class1 = model.score(x_test[y_test == 1], y_test[y_test == 1])\n",
    "    return pd.Series([accuracy_overall, accuracy_class0, accuracy_class1],\n",
    "                     index=['Overall accuracy', 'Accuracy on class 0', 'Accuracy on class 1'])\n",
    "\n",
    "def simulate_helper(filtered_df, pred_series):\n",
    "    \"\"\" Helper function for 'simulate'\n",
    "    pred_series is panda series\n",
    "    Returns mean profits \"\"\"\n",
    "    profits = []\n",
    "\n",
    "    # iterate through each article and prediction\n",
    "    for index, pred in enumerate(pred_series):\n",
    "        # extract stock prices before and after the article's publication\n",
    "        curr_share_value = filtered_df.iloc[index]['price_before']\n",
    "        new_share_value = filtered_df.iloc[index]['price_after']\n",
    "        \n",
    "        # store actual change in the stock market\n",
    "        actual = filtered_df.iloc[index]['y']\n",
    "        \n",
    "        if (pred == 1): # we expect the stock price to increase, so we buy now\n",
    "            profits += [new_share_value - curr_share_value]\n",
    "\n",
    "        elif (pred == 0): # we expect the stock price to decrease, so we sell now\n",
    "             profits += [curr_share_value - new_share_value]\n",
    "\n",
    "    return np.mean(profits)\n",
    "\n",
    "def simulate(bag_to_use, model_to_use, filtered_df):\n",
    "    \"\"\" Simulates the buying and selling process if a user bought/sell a stock according to the predictions\n",
    "    of the model (model_to_use given) the headlines (bag_to_use)\n",
    "    Returns average daily profits if we must buy or sell a share after reading that article's headline on the\n",
    "        day the article is published (or the nearest business day after), as well as the profit generated if we bought\n",
    "        or sell randomly \"\"\"\n",
    "    \n",
    "    filtered_df['random'] = ''\n",
    "    filtered_df['model'] = ''\n",
    "\n",
    "    for i in range (0, filtered_df.shape[0]):\n",
    "        filtered_df['random'].values[i] = round(np.random.rand())\n",
    "\n",
    "    filtered_df['model'] = model_to_use.predict(bag_to_use)\n",
    "\n",
    "#     print 'Random (50/50):'\n",
    "#     print 'Model prediction:'\n",
    "    return simulate_helper(filtered_df, filtered_df['model']), simulate_helper(filtered_df, filtered_df['random'])\n",
    "#     print 'Always buy:'\n",
    "#     simulate_helper(filtered_df, np.zeros((filtered_df.shape[0], 1)) + 1)\n",
    "#     print 'Always sell:'\n",
    "#     simulate_helper(filtered_df, np.zeros((filtered_df.shape[0], 1)))\n",
    "#     print 'Fortune teller:'\n",
    "#     simulate_helper(filtered_df, filtered_df['y'])\n",
    "\n",
    "def simulate_with_parems(occ_thresh, pca, n_est, m_dep, m_leaf, filtered_df_set):   \n",
    "    \"\"\" occ_thresh = occurance threshold\n",
    "    pca = pca dimentiality\n",
    "    n_est = num estimators\n",
    "    m_dep = max deptch\n",
    "    m_leaf = max leaf nodes \"\"\"\n",
    "    \n",
    "    rf = RandomForest(n_estimators = n_est, max_depth = m_dep, max_leaf_nodes = m_leaf, class_weight = 'balanced')\n",
    "    if(pca != 0):\n",
    "        bag_to_use = make_pca(vectorize(filtered_df_set['headline'],occ_thresh)[0],pca)\n",
    "    else:\n",
    "        bag_to_use = vectorize(filtered_df_set['headline'],int(occ_thresh))[0]\n",
    "    y = filtered_df['y'].values\n",
    "    x_train, x_test, y_train, y_test = train_test_split(bag_to_use, \n",
    "                                                            y, \n",
    "                                                            test_size = 0.4) \n",
    "    rf.fit(x_train,y_train)\n",
    "\n",
    "    return simulate(bag_to_use, rf, filtered_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel('10_year_data.xlsx')\n",
    "df['pub_date'] = pd.DatetimeIndex(df['pub_date']).normalize()\n",
    "print 'Number of total articles:', df.shape[0]\n",
    "filtered_df = filter_data(df, word_filter = ['Apple', 'AAPL', 'iPhone', 'iPod', 'MacBook'], option = 'headline')\n",
    "print 'Number of articles after filtering:', filtered_df.shape[0]\n",
    "\n",
    "# load stock price\n",
    "stock_price = pd.read_csv('12-4-06-to-12-3-16-Quotes.csv', parse_dates = [0], keep_date_col = True, encoding = 'cp1252')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filtered_df = pd.read_excel('10_year_filtered_data.xlsx')\n",
    "\n",
    "filtered_df_pos = deepcopy(filtered_df)\n",
    "filtered_df_stem = deepcopy(filtered_df)\n",
    "filtered_df_pos_stem = deepcopy(filtered_df)\n",
    "\n",
    "filtered_df_pos['headline'] = filter_misc(filtered_df_pos['headline'], pos = ['NN', 'JJ'], stem = False)\n",
    "filtered_df_stem['headline'] = filter_misc(filtered_df_stem['headline'], pos = None, stem = True)\n",
    "filtered_df_pos_stem['headline'] = filter_misc(filtered_df_pos_stem['headline'], pos = ['NN', 'JJ'], stem = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_lt_ = pd.read_excel('best_rf_thresh_lt.xlsx')\n",
    "_gt_ = pd.read_excel('best_rf_thresh_gt.xlsx')\n",
    "\n",
    "lt_profit_scores = []\n",
    "gt_profit_scores = []\n",
    "\n",
    "for filtered_df_sub, it_item, score_list in zip([label_data(filtered_df, threshold_quantity=0.005, threshold_label = 'lt'), label_data(filtered_df, threshold_quantity=0.005, threshold_label = 'gt')], [_lt_, _gt_],[lt_profit_scores, gt_profit_scores]):\n",
    "    \n",
    "    filtered_df = filtered_df_sub\n",
    "    \n",
    "    for index, row in it_item.iterrows():\n",
    "        if pd.isnull(row['Number of Trees']) or pd.isnull(row['Depth']) is None or pd.isnull(row['Max Leaf Nodes']) is None:\n",
    "            score_list += [[row['ID'],0]]\n",
    "            continue\n",
    "\n",
    "        df_to_use = None\n",
    "        if row['POS'] == 0 and row['Stem'] == 0:\n",
    "            df_to_use = filtered_df\n",
    "        elif row['POS'] == 1 and row['Stem'] == 0:\n",
    "            df_to_use = filtered_df_pos\n",
    "        elif row['POS'] == 0 and row['Stem'] == 1:\n",
    "            df_to_use = filtered_df_stem\n",
    "        else:\n",
    "            df_to_use = filtered_df_pos_stem\n",
    "\n",
    "        if row['No PCA/PCA(2)'] == 1.0:\n",
    "            pca_parem = 2\n",
    "        elif row['No PCA/PCA(2)'] != 0.0:\n",
    "            pca_parem = int(row['No PCA/PCA(2)'])\n",
    "        else:\n",
    "            pca_parem = 0\n",
    "\n",
    "        score = []\n",
    "        for _ in range(0,50):\n",
    "            score += [simulate_with_parems(int(row['Occurance Threshold']), int(pca_parem), int(row['Number of Trees']), int(row['Depth']), int(row['Max Leaf Nodes']), df_to_use)]\n",
    "        score_list += [[row['ID'],np.mean(score)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lt_profit_scores = np.array(lt_profit_scores)\n",
    "gt_profit_scores = np.array(gt_profit_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model LT:  [ 30.           0.17733892]\n",
      "Best Model GT:  [ 19.           0.11460199]\n"
     ]
    }
   ],
   "source": [
    "print \"Best Model LT: \", lt_profit_scores[np.where(lt_profit_scores==np.max(lt_profit_scores[:,1]))[0],:][0]\n",
    "print \"Best Model GT: \", gt_profit_scores[np.where(gt_profit_scores==np.max(gt_profit_scores[:,1]))[0],:][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filtered_df = label_data(filtered_df, threshold_quantity=0.005, threshold_label = 'gt')\n",
    "gt_best_model_scores = []\n",
    "for _ in range(0,100):\n",
    "    results = simulate_with_parems(1,1,130,9,9,filtered_df_pos_stem)\n",
    "    gt_best_model_scores += [[results[0],results[1]]]\n",
    "gt_best_model_scores = np.array(gt_best_model_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filtered_df = label_data(filtered_df, threshold_quantity=0.005, threshold_label = 'lt')\n",
    "lt_best_model_scores = []\n",
    "for _ in range(0,100):\n",
    "    results = simulate_with_parems(1,0,70,7,4,filtered_df_pos)\n",
    "    lt_best_model_scores += [[results[0], results[1]]]\n",
    "lt_best_model_scores = np.array(lt_best_model_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(gt_best_model_scores[:,0], bins=15, alpha=0.5, color='g')\n",
    "plt.xlabel('Profit scores')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Model 19')\n",
    "plt.axvline(np.mean(gt_best_model_scores[:,0]), linewidth=2, color='b')\n",
    "plt.axvline(np.mean(gt_best_model_scores[:,1]), linewidth=2, color='black')\n",
    "plt.axvline(0.0743334265734, linewidth=2, color='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(lt_best_model_scores[:,0], bins=15, alpha=0.5, color='r')\n",
    "plt.xlabel('Profit scores')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Model 30')\n",
    "plt.axvline(np.mean(lt_best_model_scores[:,0]), linewidth=2, color='b')\n",
    "plt.axvline(np.mean(lt_best_model_scores[:,1]), linewidth=2, color='black')\n",
    "plt.axvline(0.0743334265734, linewidth=2, color='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaboost #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier as AdaBoost\n",
    "from sklearn.tree import DecisionTreeClassifier as DecisionTree\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "boost_score = []\n",
    "boost_score_train = []\n",
    "\n",
    "vectorized_1gram, vectorized_1gram_names = vectorize(filtered_df['headline'])\n",
    "vectorized_pos_1gram, vectorized_pos_1gram_names = vectorize(filtered_df_pos['headline'])\n",
    "vectorized_stem_1gram, vectorized_stem_1gram_names = vectorize(filtered_df_stem['headline'])\n",
    "vectorized_pos_stem_1gram, vectorized_pos_stem_1gram_names = vectorize(filtered_df_pos_stem['headline'])\n",
    "\n",
    "# project to the data onto the two axes\n",
    "bag_words_pca_1gram = make_pca(vectorized_1gram, 2)\n",
    "bag_words_pca_pos_1gram = make_pca(vectorized_pos_1gram, 2)\n",
    "bag_words_pca_stem_1gram = make_pca(vectorized_stem_1gram, 2)\n",
    "bag_words_pca_pos_stem_1gram = make_pca(vectorized_pos_stem_1gram, 2)\n",
    "\n",
    "filtered_df = label_data(filtered_df, threshold_quantity=0.005, threshold_label = 'gt')\n",
    "\n",
    "scooore = []\n",
    "for x in np.arange(10,200,10):\n",
    "    boost = AdaBoost(DecisionTree(max_depth = 1, class_weight='balanced'), n_estimators = x)\n",
    "    bag_to_use = vectorized_pos_stem_1gram\n",
    "    y = filtered_df['y'].values\n",
    "    scores = cross_val_score(boost, bag_to_use, y, cv=5)\n",
    "    scooore += [scores.mean()]                            \n",
    "\n",
    "# print 'Test score:\\n', score(boost, x_test, np.array(y_test))\n",
    "# print 'Train score:\\n', score(boost, x_train, np.array(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(np.arange(10,200,10), scooore, c='r')\n",
    "plt.xlabel('n_estimators')\n",
    "plt.ylabel('accuracy score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "boost = AdaBoost(DecisionTree(max_depth = 1, class_weight='balanced'), n_estimators = 100)\n",
    "bag_to_use = vectorized_pos_stem_1gram\n",
    "y = filtered_df['y'].values\n",
    "\n",
    "test_scores, train_scores = [],[]\n",
    "for _ in range(0,100):\n",
    "    x_train, x_test, y_train, y_test = train_test_split(bag_to_use, \n",
    "                                                        y, \n",
    "                                                        test_size = 0.4) \n",
    "    #                                                     random_state = 42)\n",
    "    boost.fit(x_train, y_train)\n",
    "#     boost_score += boost.score(x_test, y_test)\n",
    "#     boost_score_train += boost.score(x_train, y_train)\n",
    "\n",
    "    test_scores += [score(boost, x_test, np.array(y_test)).values]\n",
    "    train_scores += [score(boost, x_train, np.array(y_train)).values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot data\n",
    "fig, ax = plt.subplots(1, 3, figsize = (20, 5))\n",
    "ax[0].hist(np.array(test_scores)[:,0], bins=40, color='b', alpha=0.5)\n",
    "ax[0].set_xlabel('Overall Scores')\n",
    "ax[0].set_ylabel('Frequency')\n",
    "ax[1].hist(np.array(test_scores)[:,1], bins=40, color='g', alpha=0.5)\n",
    "ax[1].set_xlabel('True Negative Scores')\n",
    "ax[1].set_ylabel('Frequency')\n",
    "ax[2].hist(np.array(test_scores)[:,2], bins=40, color='r', alpha=0.5)\n",
    "ax[2].set_xlabel('True Positive Scores')\n",
    "ax[2].set_ylabel('Frequency')\n",
    "fig.suptitle('AdaBoost Classifier with 100 estimators')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
