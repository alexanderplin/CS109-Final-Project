{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.corpus import stopwords\n",
    "from datetime import timedelta\n",
    "from sklearn.linear_model import LogisticRegression as LogReg\n",
    "from sklearn.linear_model import LogisticRegressionCV as LogRegCV\n",
    "import math\n",
    "import string \n",
    "from six.moves.html_parser import HTMLParser\n",
    "import urllib2\n",
    "import json\n",
    "import time\n",
    "from functools import wraps\n",
    "from copy import deepcopy\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "#%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of total articles: 655600\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel('10_year_data.xlsx')\n",
    "df['pub_date'] = pd.DatetimeIndex(df['pub_date']).normalize()\n",
    "#df['pub_date'] = pd.to_datetime(df['pub_date']).normalize()\n",
    "print 'Number of total articles:', df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_pos(x, pos):\n",
    "    return ' '.join([y for y,tag in nltk.pos_tag(nltk.word_tokenize(x)) if tag in pos])\n",
    "\n",
    "\"\"\" data = pandas dataframe with columns 'pub_date', 'headline', 'paragraph'\n",
    "    option = 'headline' or 'paragraph' (default = 'headline')\n",
    "    date_filer = (start date, end date) inclusive; else if default = None, then no filter \n",
    "    word_filter = list of words to filter OPTION by (CASE SENSITIVE); default is None\n",
    "    pos = list of parts of speech tags (default = None)\n",
    "    stem = boolean, whether or not to stem (default = False)\"\"\"\n",
    "def filter_data(data, option = 'headline', date_filter = None, word_filter = None, pos = None, stem = False):\n",
    "    h = HTMLParser()\n",
    "    # does not stem stopwords\n",
    "    stemmer = EnglishStemmer(ignore_stopwords = True)\n",
    "    \n",
    "    # filter by dates\n",
    "    if date_filter is not None:\n",
    "        start_date, end_date = date_filter\n",
    "        filtered_data = data[(data['publish_date'] >= start_date) & (data['publish_date'] <= end_date)]\n",
    "    else:\n",
    "        filtered_data = data\n",
    "        \n",
    "    \n",
    "    # filter by words\n",
    "    if word_filter is not None:\n",
    "        idx_to_drop = [] # store indices of rows that do not contain filter words\n",
    "        \n",
    "        # for every article\n",
    "        for i in range(filtered_data.shape[0]):\n",
    "            text = filtered_data.iloc[i][option]\n",
    "            \n",
    "            # iterates through each filter word\n",
    "            filter_flag = 0\n",
    "            # if there is no text (type is not string)\n",
    "            if isinstance(text, basestring) is False:\n",
    "                idx_to_drop.append(i)\n",
    "                continue\n",
    "            for word in word_filter:\n",
    "                if word in text:\n",
    "                    filter_flag = 1\n",
    "                    break\n",
    "            # if no filter words in text, drop\n",
    "            if filter_flag == 0:\n",
    "                idx_to_drop.append(i)\n",
    "            # else if being filtered and stem is true\n",
    "            elif stem == True:\n",
    "                for word in text:\n",
    "                    word = stemmer.stem(word)\n",
    "    \n",
    "    # drops rows without words in filter\n",
    "    filtered_data = filtered_data.drop(filtered_data.index[idx_to_drop])\n",
    "    \n",
    "    if pos is not None:\n",
    "        filtered_data[option].apply(extract_pos, args=(pos,))\n",
    "    \n",
    "    # remove punctuation\n",
    "    # remove html encoding puncuation from old news\n",
    "    html_encoding = ['&#8217;', '&#8212;', '&#038;', '&#8230;', '&#8220;', '&#8221;']\n",
    "    for i in range(filtered_data.shape[0]):\n",
    "        for encoding in html_encoding:\n",
    "            if encoding in filtered_data.iloc[i][option]:\n",
    "                index = filtered_data.index[i]\n",
    "                filtered_data.loc[index, option] = filtered_data.loc[index, option].replace(encoding, h.unescape(encoding))\n",
    "    \n",
    "    # remove other punctuation\n",
    "    punctuation = list(',.!@#$%^&*()\\'\\\"`:;?' + u'\\u2018' + u'\\u2019')\n",
    "    for c in punctuation:\n",
    "        filtered_data[option] = filtered_data[option].str.replace(c, '')\n",
    "    \n",
    "#     # remove stop words\n",
    "#     for i in range(filtered_data.shape[0]):\n",
    "#         headline = filtered_data.iloc[i][option]\n",
    "#         headline = headline.split()\n",
    "#         new_headline = []\n",
    "#         for word in headline:\n",
    "#             if word not in stopwords.words('english'):\n",
    "#                 new_headline += [word]\n",
    "        \n",
    "#         index = filtered_data.index[i]\n",
    "#         filtered_data.loc[index, option] = ' '.join(new_headline)\n",
    "                    \n",
    "    return filtered_data\n",
    "\n",
    "# makes bag of words given a pre-filtered DF and the option of using title or abstract\n",
    "def make_bag_of_words(df, option = 'headline'):\n",
    "    num_articles = df.shape[0]\n",
    "    words_lst = []\n",
    "    for i in range(num_articles):\n",
    "        words_lst += df.iloc[i][option].split()\n",
    "    \n",
    "    unique_words = list(set(words_lst))\n",
    "    print '\\nNumber of unique words across all text (including stopwords):', len(unique_words)\n",
    "    \n",
    "    # new predictors\n",
    "    bag_words = np.zeros((df.shape[0], len(unique_words)))\n",
    "    \n",
    "    # text of words is a list\n",
    "    for i in range(num_articles):\n",
    "        text = df.iloc[i][option].split()\n",
    "        for word in text:\n",
    "            word_col_num = unique_words.index(word)\n",
    "            bag_words[i, word_col_num] += 1\n",
    "    \n",
    "    print unique_words[:20]\n",
    "    print bag_words[:5, :20]\n",
    "    \n",
    "    return bag_words, unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles after filtering: 1430\n"
     ]
    }
   ],
   "source": [
    "filtered_df = filter_data(df, word_filter = ['Apple', 'AAPL', 'iPhone', 'iPod', 'MacBook'], option = 'headline')\n",
    "print 'Number of articles after filtering:', filtered_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
